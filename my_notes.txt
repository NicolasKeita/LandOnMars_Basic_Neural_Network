# TODO remove all of this

# RL is
#  - Optimization
#  - Delayed consequences
#  - Exploration
#  - Generalization (my agent has been trained with a specific environment,
#       but I'd like it to be effective on future unknown environment as well
#

# TODO move this info somewhere else:

# I have access to a model (given an action, I am able to predict what next state will be), I also am able to observe the next state but more importily I am able to predict,
# I also have a reward model, able to query a state and assign a reward to it without having to observe it. I can predict it.
# I can represent my model in different ways :
# table lookup model...
# TODO add more, stanford RL lecture 16
# a reward function (able to know immediate reward, which is 0 all the time expect when actually landing) but
# I don't have access to a Value Function (no quick way to know the expected reward sum)
# and there are (finite) actions the agent is able take.

# Despite having a model, I use model-free RL.
#
# ------------------            Metaheuristic        -------------------------#
# ---           Metaheuristic implicit        -       ------------------------#
# Seems like genetic algorithm is very good when there are multiple solutions (able quickly visit local-optimas)
# ---           Metaheuristic explicit        -       ------------------------#

# ------------------            Model-free RL        -------------------------#
# Usage of model-free because I can observe the next state after every action (the known model is allowing me to observe the next state after doing any action)
# Optimization - Policy Search
#   Policy Gradient methods vs Metaheuristic?
#       Particle Swarm Optimization algorithm
#       (special mention for Genetic algorithm,
#       generating hundreds of individuals each generation
#       felt too slow to converge)

# -------------------            Model-based RL       ------------------------#
# Only motive to use model-based RL would be that I have a model and
#   I cannot observe the states during the runs or the simulation or
#   The state space is small.
# Special mention for MCTS: that could work but,
#   I don't have the time (100ms max to compute next action) to even go over 1/100 of the tree and/or
#   I don't have the knowledge yet to generalize with weights that would fit into less than 50MB (CNN ?)

# Model-free RL is probably most suited for 90% of the problems. I guess that's why there is learning in RL.

# Planning = Compute a policy given a model - Value iteration / Policy iteration / dynamic programming - Policy Search / Q learning / dynamic programming - Approximate planning
# Planning algorithm :
# Value iteration
# Policy iteration
# Tree Search
# ...

# Control is a key aspect of this problem

# Genetic programming, linear regression, and decision trees up to
# a certain depth are examples of interpretable models, while neural networks
# and ensemble methods are considered black-boxes

# Classical control algorithm (PID / LQR / MPC) vs RL control algorithms
# bias = constant features

# Policy-Based RL
# The world outside my agent is stationary (independent of the agent actions).
